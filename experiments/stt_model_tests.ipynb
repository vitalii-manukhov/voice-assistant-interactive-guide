{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "def test_whisper(model_name):\n",
    "    model = whisper.load_model(model_name)\n",
    "\n",
    "    for speech_number in range(1, 14):\n",
    "        filepath = f\"./data/speech_{speech_number}.wav\"\n",
    "        result = model.transcribe(filepath, language=\"ru\", fp16=False)\n",
    "        print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base\n",
    "test_whisper(\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small\n",
    "test_whisper(\"small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# medium\n",
    "test_whisper(\"medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vosk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "import sys\n",
    "\n",
    "from vosk import Model, KaldiRecognizer, SetLogLevel\n",
    "\n",
    "# You can set log level to -1 to disable debug messages\n",
    "SetLogLevel(0)\n",
    "\n",
    "filepath = \"./data/speech_1_16bit.wav\"\n",
    "wf = wave.open(filepath, \"rb\")\n",
    "if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getcomptype() != \"NONE\":\n",
    "    print(\"Audio file must be WAV format mono PCM.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "model_name = \"vosk-model-small-ru-0.22\"\n",
    "model = Model(model_name=model_name, lang=\"ru\")\n",
    "\n",
    "rec = KaldiRecognizer(model, wf.getframerate())\n",
    "rec.SetWords(True)\n",
    "rec.SetPartialWords(True)\n",
    "\n",
    "while True:\n",
    "    data = wf.readframes(4000)\n",
    "    if len(data) == 0:\n",
    "        break\n",
    "    if rec.AcceptWaveform(data):\n",
    "        print(rec.Result())\n",
    "    else:\n",
    "        print(rec.PartialResult())\n",
    "\n",
    "print(rec.FinalResult())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silero models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import zipfile\n",
    "import torchaudio\n",
    "from glob import glob\n",
    "\n",
    "device = torch.device('cpu')  # gpu also works, but our models are fast enough for CPU\n",
    "model, decoder, utils = torch.hub.load(repo_or_dir='snakers4/silero-models',\n",
    "                                       model='silero_stt',\n",
    "                                       language='en', # also available 'de', 'es'\n",
    "                                       device=device)\n",
    "(read_batch, split_into_batches,\n",
    " read_audio, prepare_model_input) = utils  # see function signature for details\n",
    "\n",
    "# download a single file in any format compatible with TorchAudio\n",
    "torch.hub.download_url_to_file('https://opus-codec.org/static/examples/samples/speech_orig.wav',\n",
    "                               dst ='speech_orig.wav', progress=True)\n",
    "test_files = glob('speech_orig.wav')\n",
    "batches = split_into_batches(test_files, batch_size=10)\n",
    "input = prepare_model_input(read_batch(batches[0]),\n",
    "                            device=device)\n",
    "\n",
    "output = model(input)\n",
    "for example in output:\n",
    "    print(decoder(example.cpu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nvidia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "asr_model = nemo_asr.models.EncDecRNNTBPEModel.from_pretrained(\"nvidia/stt_ru_conformer_transducer_large\")\n",
    "\n",
    "filepath = \"./data/speech_1_16bit.wav\"\n",
    "asr_model.transcribe([filepath])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wav2vec2-large-xlsr-53-russian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingsound import SpeechRecognitionModel\n",
    "\n",
    "model = SpeechRecognitionModel(\"jonatasgrosman/wav2vec2-large-xlsr-53-russian\")\n",
    "audio_paths = [\"./data/speech_5_16bit.wav\"]\n",
    "\n",
    "transcriptions = model.transcribe(audio_paths)\n",
    "\n",
    "print(transcriptions[0][\"transcription\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sneakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import zipfile\n",
    "import torchaudio\n",
    "from glob import glob\n",
    "\n",
    "device = torch.device('cpu')  # gpu also works, but our models are fast enough for CPU\n",
    "model, decoder, utils = torch.hub.load(repo_or_dir='snakers4/silero-models',\n",
    "                                       model='silero_stt',\n",
    "                                       language='ru', # also available 'de', 'es'\n",
    "                                       device=device)\n",
    "(read_batch, split_into_batches,\n",
    " read_audio, prepare_model_input) = utils  # see function signature for details\n",
    "\n",
    "# download a single file in any format compatible with TorchAudio\n",
    "torch.hub.download_url_to_file('https://opus-codec.org/static/examples/samples/speech_orig.wav',\n",
    "                               dst ='speech_orig.wav', progress=True)\n",
    "filename = \"./data/speech_1.wav\"\n",
    "test_files = glob(filename)\n",
    "batches = split_into_batches(test_files, batch_size=10)\n",
    "input = prepare_model_input(read_batch(batches[0]),\n",
    "                            device=device)\n",
    "\n",
    "output = model(input)\n",
    "for example in output:\n",
    "    print(decoder(example.cpu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nvidia nemo or Whisper small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"small\")\n",
    "\n",
    "for speech_number in range(1, 14):\n",
    "    filepath = f\"data/speech_{speech_number}_pcm16.wav\"\n",
    "    result = model.transcribe(filepath, language=\"ru\", fp16=False)\n",
    "    print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nemo.collections.asr as nemo_asr\n",
    "asr_model = nemo_asr.models.EncDecRNNTBPEModel.from_pretrained(\"nvidia/stt_ru_conformer_transducer_large\")\n",
    "\n",
    "for speech_number in range(1, 14):\n",
    "    filepath = f\"data/speech_{speech_number}_mono.wav\"\n",
    "    result = asr_model.transcribe([filepath])\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
